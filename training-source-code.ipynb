{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"background: linear-gradient(to right, #2b5876, #4e4376);\" align=\"center\">\n","\n","# <span style=\"color: white; padding: 10px 20px; border-radius: 10px;\">üîç Segmentation Faciale par Deep Learning</span>\n","\n","## <span s style=\"color: white;\">Analyse Comparative des Architectures pour la D√©tection des Traits du Visage</span>\n","___\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["## *Downloading the LAPA Face Parsing Dataset*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-01-18T23:05:05.941487Z","iopub.status.busy":"2025-01-18T23:05:05.941228Z","iopub.status.idle":"2025-01-18T23:05:06.215418Z","shell.execute_reply":"2025-01-18T23:05:06.214538Z","shell.execute_reply.started":"2025-01-18T23:05:05.941464Z"},"trusted":true},"outputs":[],"source":["import kagglehub\n","import shutil\n","import os\n","# Download latest version\n","path = kagglehub.dataset_download(\"kiranraghavendrauci/lapa-face-parsing-dataset\")\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"markdown","metadata":{},"source":["## *Building and Training Deep Learning Models for Semantic Segmentation*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-01-18T23:06:28.021851Z","iopub.status.busy":"2025-01-18T23:06:28.021623Z","iopub.status.idle":"2025-01-19T01:30:11.419794Z","shell.execute_reply":"2025-01-19T01:30:11.418610Z","shell.execute_reply.started":"2025-01-18T23:06:28.021830Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from glob import glob\n","from tensorflow.keras.layers import Conv2D, UpSampling2D, Conv2DTranspose, Concatenate, Input, Activation, BatchNormalization, Resizing, DepthwiseConv2D, GlobalAveragePooling2D, Dense, Lambda, AveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n","from tensorflow.keras.mixed_precision import set_global_policy\n","from tensorflow.keras import backend as K\n","\n","# Activer l'entra√Ænement avec pr√©cision mixte\n","set_global_policy(\"mixed_float16\")\n","\n","# R√©duire les logs TensorFlow\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","\n","# Cr√©er un r√©pertoire si n√©cessaire\n","def create_dir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","# Chargement et pr√©traitement des donn√©es\n","def load_dataset(path):\n","    train_x = sorted(glob(os.path.join(path, \"train\", \"images\", \"*.jpg\")))\n","    train_y = sorted(glob(os.path.join(path, \"train\", \"labels\", \"*.png\")))\n","\n","    valid_x = sorted(glob(os.path.join(path, \"val\", \"images\", \"*.jpg\")))\n","    valid_y = sorted(glob(os.path.join(path, \"val\", \"labels\", \"*.png\")))\n","\n","    return (train_x, train_y), (valid_x, valid_y)\n","\n","def read_image_mask(x, y, image_h, image_w):\n","    \"\"\" Charger et redimensionner l'image et le masque \"\"\"\n","    x = cv2.imread(x, cv2.IMREAD_COLOR)\n","    x = cv2.resize(x, (image_w, image_h))\n","    x = x / 255.0\n","    x = x.astype(np.float32)\n","\n","    y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n","    y = cv2.resize(y, (image_w, image_h))\n","    y = y.astype(np.int32)\n","    return x, y\n","\n","def preprocess(x, y, image_h, image_w, num_classes):\n","    def f(x, y):\n","        x = x.decode()\n","        y = y.decode()\n","        return read_image_mask(x, y, image_h, image_w)\n","\n","    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n","    mask = tf.one_hot(mask, num_classes)\n","    image.set_shape([image_h, image_w, 3])\n","    mask.set_shape([image_h, image_w, num_classes])\n","    return image, mask\n","\n","def tf_dataset(X, Y, batch, image_h, image_w, num_classes):\n","    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n","    ds = ds.shuffle(buffer_size=5000).map(lambda x, y: preprocess(x, y, image_h, image_w, num_classes))\n","    ds = ds.batch(batch).prefetch(2)\n","    return ds\n","\n","# M√©triques d'√©valuation\n","def iou(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    intersection = K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    union = K.sum(K.cast(K.not_equal(y_true, y_pred), dtype=\"float32\"))\n","    return intersection / (union + intersection + K.epsilon())\n","\n","\n","def dice_coefficient(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    numerator = 2 * K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    denominator = K.sum(K.cast(K.not_equal(y_true, y_pred), dtype=\"float32\")) + K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    return numerator / (denominator + K.epsilon())\n","\n","def precision(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    true_positives = K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    predicted_positives = K.sum(K.cast(K.greater(y_pred, 0), dtype=\"float32\"))\n","    return true_positives / (predicted_positives + K.epsilon())\n","\n","def recall(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    true_positives = K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    actual_positives = K.sum(K.cast(K.greater(y_true, 0), dtype=\"float32\"))\n","    return true_positives / (actual_positives + K.epsilon())\n","\n","# D√©finition des mod√®les\n","def build_unet(input_shape, num_classes):\n","    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n","\n","    # Extraire les couches d'encodeur\n","    s1 = base_model.get_layer(\"block_1_expand_relu\").output\n","    s2 = base_model.get_layer(\"block_3_expand_relu\").output\n","    s3 = base_model.get_layer(\"block_6_expand_relu\").output\n","    s4 = base_model.get_layer(\"block_13_expand_relu\").output\n","    b1 = base_model.get_layer(\"block_16_project\").output\n","\n","    # D√©codeur U-Net\n","    def decoder_block(inputs, skip, num_filters):\n","        x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n","        x = Concatenate()([x, skip])\n","        x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        return x\n","\n","    d1 = decoder_block(b1, s4, 512)\n","    d2 = decoder_block(d1, s3, 256)\n","    d3 = decoder_block(d2, s2, 128)\n","    d4 = decoder_block(d3, s1, 64)\n","\n","    # Ajuster la taille de sortie pour qu'elle corresponde √† l'entr√©e\n","    outputs = UpSampling2D(size=(2, 2))(d4)\n","    outputs = Conv2D(num_classes, 1, activation=\"softmax\", padding=\"same\")(outputs)\n","\n","    model = Model(inputs=base_model.input, outputs=outputs)\n","    return model\n","\n","def build_pspnet(input_shape, num_classes):\n","    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n","    \n","    # Encoder\n","    b1 = base_model.get_layer(\"block_16_project\").output\n","    \n","    # Pyramid Pooling Module\n","    def psp_block(inputs, num_filters, pool_size):\n","        x = AveragePooling2D(pool_size=pool_size, strides=pool_size, padding='same')(inputs)\n","        x = Conv2D(num_filters, 1, padding='same', use_bias=False)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        target_size = (inputs.shape[1], inputs.shape[2])\n","        x = UpSampling2D(size=(pool_size[0], pool_size[1]))(x)\n","        return x\n","    \n","    \n","    input_h = input_shape[0]\n","    feature_map_size = input_h // 32  \n","    \n","    pool_sizes = [\n","        (feature_map_size, feature_map_size),  # Pooling global\n","        (feature_map_size // 2, feature_map_size // 2),  # Taille 1/2\n","        (feature_map_size // 3, feature_map_size // 3),  # Taille 1/3\n","        (feature_map_size // 6, feature_map_size // 6)   # Taille 1/6\n","    ]\n","    \n","    # Module de Pooling Pyramidale\n","    pyramid_features = [b1]\n","    for pool_size in pool_sizes:\n","        pyramid_features.append(psp_block(b1, 256, pool_size))\n","\n","    # Concat√©ner toutes les cartes de caract√©ristiques\n","    x = Concatenate()(pyramid_features)\n","\n","    # Convolutions finales\n","    x = Conv2D(512, 3, padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Upsampling progressif pour correspondre aux dimensions d'entr√©e\n","    # Premi√®re upsampling (8x8 -> 16x16)\n","    x = Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Deuxi√®me upsampling (16x16 -> 32x32)\n","    x = Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Troisi√®me upsampling (32x32 -> 64x64)\n","    x = Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Quatri√®me upsampling (64x64 -> 128x128)\n","    x = Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Upsampling final (128x128 -> 256x256)\n","    x = Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Ajustements finaux et sortie\n","    x = Conv2D(64, 3, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x)\n","\n","    model = Model(inputs=base_model.input, outputs=outputs)\n","    return model\n","\n","\n","\n","\n","def build_segnet(input_shape, num_classes):\n","    inputs = Input(shape=input_shape)\n","\n","    # Encoder\n","    def encoder_block(inputs, num_filters, pool_size=(2, 2)):\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(inputs)\n","        x = BatchNormalization()(x)\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(x)\n","        x = BatchNormalization()(x)\n","        p = tf.keras.layers.MaxPool2D(pool_size=pool_size, strides=pool_size)(x)\n","        return x, p\n","    \n","    enc1, p1 = encoder_block(inputs, 64)\n","    enc2, p2 = encoder_block(p1, 128)\n","    enc3, p3 = encoder_block(p2, 256)\n","    enc4, p4 = encoder_block(p3, 512)\n","    enc5, p5 = encoder_block(p4, 512)\n","    \n","    # Decoder\n","    def decoder_block(inputs, skip, num_filters, up_size=(2, 2)):\n","        x = UpSampling2D(size=up_size)(inputs)\n","        x = Concatenate()([x, skip])\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(x)\n","        x = BatchNormalization()(x)\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(x)\n","        x = BatchNormalization()(x)\n","        return x\n","    \n","    dec1 = decoder_block(p5, enc5, 512)\n","    dec2 = decoder_block(dec1, enc4, 512)\n","    dec3 = decoder_block(dec2, enc3, 256)\n","    dec4 = decoder_block(dec3, enc2, 128)\n","    dec5 = decoder_block(dec4, enc1, 64)\n","\n","    # Output\n","    outputs = Conv2D(num_classes, 1, activation=\"softmax\", padding=\"same\")(dec5)\n","    \n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Fonction d'entra√Ænement\n","def train_model(model, train_ds, valid_ds, model_path, csv_path, num_epochs, lr):\n","    callbacks = [\n","        ModelCheckpoint(model_path, verbose=1, save_best_only=True, monitor=\"val_loss\"),\n","        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n","        CSVLogger(csv_path, append=True),\n","        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n","    ]\n","\n","    model.compile(\n","        loss=\"categorical_crossentropy\",\n","        optimizer=tf.keras.optimizers.Adam(lr),\n","        metrics=[iou, dice_coefficient, precision, recall]\n","    )\n","\n","    model.fit(\n","        train_ds,\n","        validation_data=valid_ds,\n","        epochs=num_epochs,\n","        callbacks=callbacks\n","    )\n","    model.save(model_path)\n","\n","def load_segmentation_models(model_dir):\n","  \"\"\"Charge les mod√®les de segmentation √† partir du r√©pertoire sp√©cifi√©.\"\"\"\n","  unet_model = tf.keras.models.load_model(os.path.join(model_dir, \"unet_model.keras\"), custom_objects={'iou': iou, 'dice_coefficient': dice_coefficient, 'precision': precision, 'recall': recall})\n","  pspnet_model = tf.keras.models.load_model(os.path.join(model_dir, \"pspnet_model.keras\"), custom_objects={'iou': iou, 'dice_coefficient': dice_coefficient, 'precision': precision, 'recall': recall})\n","  segnet_model = tf.keras.models.load_model(os.path.join(model_dir, \"segnet_model.keras\"), custom_objects={'iou': iou, 'dice_coefficient': dice_coefficient, 'precision': precision, 'recall': recall})\n"," \n","\n","  assert unet_model is not None, \"Erreur lors du chargement du mod√®le U-Net.\"\n","  assert pspnet_model is not None, \"Erreur lors du chargement du mod√®le PSPNet.\"\n","  assert segnet_model is not None, \"Erreur lors du chargement du mod√®le SegNet.\"\n","  print(\"Tous les mod√®les sont correctement charg√©s.\")\n","\n","  return unet_model, pspnet_model, segnet_model\n","\n","\n","def load_and_preprocess_image(image_path, image_h=256, image_w=256):\n","    \"\"\"Charge, redimensionne et normalise l'image.\"\"\"\n","    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","    image = cv2.resize(image, (image_w, image_h))\n","    image = image / 255.0\n","    image = image.astype(np.float32)\n","    return image\n","\n","def display_mask(prediction, ax, cmap='viridis'):\n","    \"\"\"Affiche le masque de segmentation.\"\"\"\n","    mask = np.argmax(prediction, axis=-1)\n","    ax.imshow(mask, cmap=cmap)\n","    ax.axis('off')\n","\n","def visualize_predictions(models, model_names, dataset_path, num_images=4, use_random_images=False):\n","    \"\"\"Visualise les pr√©dictions de plusieurs mod√®les.\"\"\"\n","    if use_random_images:\n","      # Choisir al√©atoirement des images\n","      all_image_paths = sorted(glob(os.path.join(dataset_path, \"val\", \"images\", \"*.jpg\")))\n","      test_image_paths = np.random.choice(all_image_paths, size=num_images, replace=False)\n","    else:\n","      # Utiliser les premi√®res images du dossier\n","      test_image_paths = sorted(glob(os.path.join(dataset_path, \"val\", \"images\", \"*.jpg\")))[:num_images]\n","\n","    # Affichage des pr√©dictions\n","    fig, axes = plt.subplots(num_images, len(models) + 1, figsize=(5*(len(models)+1), 5 * num_images))\n","    fig.suptitle(\"Comparaison des pr√©dictions des mod√®les\", fontsize=16)\n","\n","    for i, image_path in enumerate(test_image_paths):\n","        # Charger et pr√©traiter l'image\n","        image = load_and_preprocess_image(image_path)\n","        image = np.expand_dims(image, axis=0)  # Ajouter une dimension pour le batch\n","        \n","        # Afficher l'image originale\n","        axes[i, 0].imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n","        axes[i, 0].axis('off')\n","        axes[i, 0].set_title(\"Image originale\")\n","        \n","        # Faire la pr√©diction et afficher le r√©sultat\n","        for j, (model, model_name) in enumerate(zip(models, model_names)):\n","            prediction = model.predict(image)\n","            display_mask(prediction[0], axes[i, j + 1])  # prediction[0] car predict retourne un batch de taille 1\n","            axes[i, j + 1].set_title(f\"Masque pr√©dit ({model_name})\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Programme principal\n","if __name__ == \"__main__\":\n","    # Hyperparam√®tres\n","    image_h, image_w = 256, 256  # R√©duction de la taille des images\n","    num_classes = 11\n","    batch_size = 8\n","    lr = 1e-4\n","    num_epochs = 10\n","\n","    # Chemins\n","    dataset_path = \"/kaggle/input/lapa-face-parsing-dataset/LaPa\"\n","    create_dir(\"files\")\n","\n","    # Chargement du dataset\n","    (train_x, train_y), (valid_x, valid_y) = load_dataset(dataset_path)\n","\n","    # Cr√©ation des datasets TensorFlow\n","    train_ds = tf_dataset(train_x, train_y, batch_size, image_h, image_w, num_classes)\n","    valid_ds = tf_dataset(valid_x, valid_y, batch_size, image_h, image_w, num_classes)\n","\n","\n","    # Mod√®les\n","    models = {\n","        \"unet\": build_unet,\n","        \"pspnet\": build_pspnet,\n","        \"segnet\": build_segnet,\n","    }\n","\n","    # Entra√Ænement de chaque mod√®le\n","    for model_name, model_builder in models.items():\n","        print(f\"Entra√Ænement de {model_name}...\")\n","        model = model_builder((image_h, image_w, 3), num_classes)\n","        model_path = os.path.join(\"files\", f\"{model_name}_model.keras\")\n","        csv_path = os.path.join(\"files\", f\"{model_name}_data.csv\")\n","        train_model(model, train_ds, valid_ds, model_path, csv_path, num_epochs, lr)"]},{"cell_type":"markdown","metadata":{},"source":["## *Visualisation des Pr√©dictions des Mod√®les de Segmentation Faciale*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T01:51:36.848019Z","iopub.status.busy":"2025-01-19T01:51:36.847663Z","iopub.status.idle":"2025-01-19T01:52:07.407943Z","shell.execute_reply":"2025-01-19T01:52:07.406970Z","shell.execute_reply.started":"2025-01-19T01:51:36.847992Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","def load_and_preprocess_image(image_path, image_h=256, image_w=256):\n","    \"\"\"Charge, redimensionne et normalise l'image.\"\"\"\n","    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","    image = cv2.resize(image, (image_w, image_h))\n","    image = image / 255.0\n","    image = image.astype(np.float32)\n","    return image\n","\n","def display_mask(prediction, ax, cmap='viridis', num_classes=11):\n","    \"\"\"Affiche le masque de segmentation avec une palette de couleurs adapt√©e.\"\"\"\n","    mask = np.argmax(prediction, axis=-1)\n","    \n","    # Cr√©er une palette de couleurs personnalis√©e pour les diff√©rentes parties du visage\n","    colors = plt.cm.get_cmap('tab20')(np.linspace(0, 1, num_classes))\n","    custom_cmap = plt.cm.colors.ListedColormap(colors)\n","    \n","    im = ax.imshow(mask, cmap=custom_cmap, vmin=0, vmax=num_classes-1)\n","    ax.axis('off')\n","    return im\n","\n","def visualize_predictions(models, model_names, image_paths, save_path=None):\n","    \"\"\"\n","    Visualise les pr√©dictions de plusieurs mod√®les.\n","    \n","    Args:\n","        models: Liste des mod√®les √† comparer\n","        model_names: Liste des noms des mod√®les\n","        image_paths: Liste des chemins d'images √† tester\n","        save_path: Chemin o√π sauvegarder les visualisations (optionnel)\n","    \"\"\"\n","    num_images = len(image_paths)\n","    \n","    # Cr√©ation de la figure avec une taille adapt√©e\n","    fig, axes = plt.subplots(num_images, len(models) + 1, \n","                            figsize=(5*(len(models)+1), 5 * num_images))\n","    fig.suptitle(\"Comparaison des pr√©dictions des mod√®les de segmentation\", \n","                 fontsize=16, y=1.02)\n","    \n","    # Ajuster axes pour une seule image\n","    if num_images == 1:\n","        axes = axes.reshape(1, -1)\n","\n","    # Pour chaque image\n","    for i, image_path in enumerate(image_paths):\n","        # Charger et pr√©traiter l'image\n","        image = load_and_preprocess_image(image_path)\n","        image_batch = np.expand_dims(image, axis=0)\n","        \n","        # Afficher l'image originale\n","        axes[i, 0].imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n","        axes[i, 0].axis('off')\n","        axes[i, 0].set_title(\"Image originale\", pad=10)\n","        \n","        # Faire la pr√©diction avec chaque mod√®le\n","        for j, (model, model_name) in enumerate(zip(models, model_names)):\n","            prediction = model.predict(image_batch, verbose=0)\n","            im = display_mask(prediction[0], axes[i, j + 1])\n","            axes[i, j + 1].set_title(f\"{model_name}\", pad=10)\n","    \n","    # Ajuster l'espacement\n","    plt.tight_layout()\n","    \n","    # Ajouter une colorbar pour la l√©gende\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n","    cbar = plt.colorbar(im, cax=cbar_ax)\n","    cbar.set_label('Classes de segmentation', rotation=270, labelpad=15)\n","    \n","    # Sauvegarder si un chemin est sp√©cifi√©\n","    if save_path:\n","        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n","        print(f\"Visualisation sauvegard√©e dans {save_path}\")\n","    \n","    plt.show()\n","\n","# Fonction pour charger les mod√®les\n","def load_segmentation_models(model_dir):\n","    \"\"\"Charge les mod√®les de segmentation √† partir du r√©pertoire sp√©cifi√©.\"\"\"\n","    # D√©finir les objets personnalis√©s pour le chargement\n","    custom_objects = {\n","        'iou': iou,\n","        'dice_coefficient': dice_coefficient,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","    \n","    # Charger les mod√®les\n","    unet_model = tf.keras.models.load_model(\n","        os.path.join(model_dir, \"unet_model.keras\"),\n","        custom_objects=custom_objects\n","    )\n","    \n","    pspnet_model = tf.keras.models.load_model(\n","        os.path.join(model_dir, \"pspnet_model.keras\"),\n","        custom_objects=custom_objects\n","    )\n","    \n","    segnet_model = tf.keras.models.load_model(\n","        os.path.join(model_dir, \"segnet_model.keras\"),\n","        custom_objects=custom_objects\n","    )\n","\n","    print(\"Tous les mod√®les sont correctement charg√©s.\")\n","    return unet_model, pspnet_model, segnet_model\n","\n","# Code principal pour l'utilisation\n","def main():\n","    model_dir = \"files\"\n","    dataset_path = \"/kaggle/input/lapa-face-parsing-dataset/LaPa\" \n","    # Charger les mod√®les\n","    unet_model, pspnet_model, segnet_model = load_segmentation_models(model_dir)\n","    models = [unet_model, pspnet_model, segnet_model]\n","    model_names = [\"U-Net\", \"PSPNet\", \"SegNet\"]\n","    \n","    test_images = sorted(glob(os.path.join(dataset_path, \"val\", \"images\", \"*.jpg\")))[5:6]\n","    \n","    results_dir = \"resultats_segmentation\"\n","    if not os.path.exists(results_dir):\n","        os.makedirs(results_dir)\n","    \n","    # Visualiser les pr√©dictions\n","    visualize_predictions(\n","        models,\n","        model_names,\n","        test_images,\n","        save_path=os.path.join(results_dir, \"comparaison_modeles.png\") # Sauvegarder l'image\n","    )\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4705085,"sourceId":7992059,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
