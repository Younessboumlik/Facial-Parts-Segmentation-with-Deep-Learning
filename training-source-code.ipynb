{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div style=\"background: linear-gradient(to right, #2b5876, #4e4376);\" align=\"center\">\n","\n","# <span style=\"color: white; padding: 10px 20px; border-radius: 10px;\">🔍 Segmentation Faciale par Deep Learning</span>\n","\n","## <span s style=\"color: white;\">Analyse Comparative des Architectures pour la Détection des Traits du Visage</span>\n","___\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["## *Downloading the LAPA Face Parsing Dataset*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-01-18T23:05:05.941487Z","iopub.status.busy":"2025-01-18T23:05:05.941228Z","iopub.status.idle":"2025-01-18T23:05:06.215418Z","shell.execute_reply":"2025-01-18T23:05:06.214538Z","shell.execute_reply.started":"2025-01-18T23:05:05.941464Z"},"trusted":true},"outputs":[],"source":["import kagglehub\n","import shutil\n","import os\n","# Download latest version\n","path = kagglehub.dataset_download(\"kiranraghavendrauci/lapa-face-parsing-dataset\")\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"markdown","metadata":{},"source":["## *Building and Training Deep Learning Models for Semantic Segmentation*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-01-18T23:06:28.021851Z","iopub.status.busy":"2025-01-18T23:06:28.021623Z","iopub.status.idle":"2025-01-19T01:30:11.419794Z","shell.execute_reply":"2025-01-19T01:30:11.418610Z","shell.execute_reply.started":"2025-01-18T23:06:28.021830Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","from glob import glob\n","from tensorflow.keras.layers import Conv2D, UpSampling2D, Conv2DTranspose, Concatenate, Input, Activation, BatchNormalization, Resizing, DepthwiseConv2D, GlobalAveragePooling2D, Dense, Lambda, AveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n","from tensorflow.keras.mixed_precision import set_global_policy\n","from tensorflow.keras import backend as K\n","\n","# Activer l'entraînement avec précision mixte\n","set_global_policy(\"mixed_float16\")\n","\n","# Réduire les logs TensorFlow\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","\n","# Créer un répertoire si nécessaire\n","def create_dir(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","# Chargement et prétraitement des données\n","def load_dataset(path):\n","    train_x = sorted(glob(os.path.join(path, \"train\", \"images\", \"*.jpg\")))\n","    train_y = sorted(glob(os.path.join(path, \"train\", \"labels\", \"*.png\")))\n","\n","    valid_x = sorted(glob(os.path.join(path, \"val\", \"images\", \"*.jpg\")))\n","    valid_y = sorted(glob(os.path.join(path, \"val\", \"labels\", \"*.png\")))\n","\n","    return (train_x, train_y), (valid_x, valid_y)\n","\n","def read_image_mask(x, y, image_h, image_w):\n","    \"\"\" Charger et redimensionner l'image et le masque \"\"\"\n","    x = cv2.imread(x, cv2.IMREAD_COLOR)\n","    x = cv2.resize(x, (image_w, image_h))\n","    x = x / 255.0\n","    x = x.astype(np.float32)\n","\n","    y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n","    y = cv2.resize(y, (image_w, image_h))\n","    y = y.astype(np.int32)\n","    return x, y\n","\n","def preprocess(x, y, image_h, image_w, num_classes):\n","    def f(x, y):\n","        x = x.decode()\n","        y = y.decode()\n","        return read_image_mask(x, y, image_h, image_w)\n","\n","    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n","    mask = tf.one_hot(mask, num_classes)\n","    image.set_shape([image_h, image_w, 3])\n","    mask.set_shape([image_h, image_w, num_classes])\n","    return image, mask\n","\n","def tf_dataset(X, Y, batch, image_h, image_w, num_classes):\n","    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n","    ds = ds.shuffle(buffer_size=5000).map(lambda x, y: preprocess(x, y, image_h, image_w, num_classes))\n","    ds = ds.batch(batch).prefetch(2)\n","    return ds\n","\n","# Métriques d'évaluation\n","def iou(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    intersection = K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    union = K.sum(K.cast(K.not_equal(y_true, y_pred), dtype=\"float32\"))\n","    return intersection / (union + intersection + K.epsilon())\n","\n","\n","def dice_coefficient(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    numerator = 2 * K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    denominator = K.sum(K.cast(K.not_equal(y_true, y_pred), dtype=\"float32\")) + K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    return numerator / (denominator + K.epsilon())\n","\n","def precision(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    true_positives = K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    predicted_positives = K.sum(K.cast(K.greater(y_pred, 0), dtype=\"float32\"))\n","    return true_positives / (predicted_positives + K.epsilon())\n","\n","def recall(y_true, y_pred):\n","    y_true = K.flatten(K.argmax(y_true, axis=-1))\n","    y_pred = K.flatten(K.argmax(y_pred, axis=-1))\n","    true_positives = K.sum(K.cast(K.equal(y_true, y_pred), dtype=\"float32\"))\n","    actual_positives = K.sum(K.cast(K.greater(y_true, 0), dtype=\"float32\"))\n","    return true_positives / (actual_positives + K.epsilon())\n","\n","# Définition des modèles\n","def build_unet(input_shape, num_classes):\n","    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n","\n","    # Extraire les couches d'encodeur\n","    s1 = base_model.get_layer(\"block_1_expand_relu\").output\n","    s2 = base_model.get_layer(\"block_3_expand_relu\").output\n","    s3 = base_model.get_layer(\"block_6_expand_relu\").output\n","    s4 = base_model.get_layer(\"block_13_expand_relu\").output\n","    b1 = base_model.get_layer(\"block_16_project\").output\n","\n","    # Décodeur U-Net\n","    def decoder_block(inputs, skip, num_filters):\n","        x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n","        x = Concatenate()([x, skip])\n","        x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        return x\n","\n","    d1 = decoder_block(b1, s4, 512)\n","    d2 = decoder_block(d1, s3, 256)\n","    d3 = decoder_block(d2, s2, 128)\n","    d4 = decoder_block(d3, s1, 64)\n","\n","    # Ajuster la taille de sortie pour qu'elle corresponde à l'entrée\n","    outputs = UpSampling2D(size=(2, 2))(d4)\n","    outputs = Conv2D(num_classes, 1, activation=\"softmax\", padding=\"same\")(outputs)\n","\n","    model = Model(inputs=base_model.input, outputs=outputs)\n","    return model\n","\n","def build_pspnet(input_shape, num_classes):\n","    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n","    \n","    # Encoder\n","    b1 = base_model.get_layer(\"block_16_project\").output\n","    \n","    # Pyramid Pooling Module\n","    def psp_block(inputs, num_filters, pool_size):\n","        x = AveragePooling2D(pool_size=pool_size, strides=pool_size, padding='same')(inputs)\n","        x = Conv2D(num_filters, 1, padding='same', use_bias=False)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        target_size = (inputs.shape[1], inputs.shape[2])\n","        x = UpSampling2D(size=(pool_size[0], pool_size[1]))(x)\n","        return x\n","    \n","    \n","    input_h = input_shape[0]\n","    feature_map_size = input_h // 32  \n","    \n","    pool_sizes = [\n","        (feature_map_size, feature_map_size),  # Pooling global\n","        (feature_map_size // 2, feature_map_size // 2),  # Taille 1/2\n","        (feature_map_size // 3, feature_map_size // 3),  # Taille 1/3\n","        (feature_map_size // 6, feature_map_size // 6)   # Taille 1/6\n","    ]\n","    \n","    # Module de Pooling Pyramidale\n","    pyramid_features = [b1]\n","    for pool_size in pool_sizes:\n","        pyramid_features.append(psp_block(b1, 256, pool_size))\n","\n","    # Concaténer toutes les cartes de caractéristiques\n","    x = Concatenate()(pyramid_features)\n","\n","    # Convolutions finales\n","    x = Conv2D(512, 3, padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Upsampling progressif pour correspondre aux dimensions d'entrée\n","    # Première upsampling (8x8 -> 16x16)\n","    x = Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Deuxième upsampling (16x16 -> 32x32)\n","    x = Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Troisième upsampling (32x32 -> 64x64)\n","    x = Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Quatrième upsampling (64x64 -> 128x128)\n","    x = Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Upsampling final (128x128 -> 256x256)\n","    x = Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    # Ajustements finaux et sortie\n","    x = Conv2D(64, 3, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    \n","    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x)\n","\n","    model = Model(inputs=base_model.input, outputs=outputs)\n","    return model\n","\n","\n","\n","\n","def build_segnet(input_shape, num_classes):\n","    inputs = Input(shape=input_shape)\n","\n","    # Encoder\n","    def encoder_block(inputs, num_filters, pool_size=(2, 2)):\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(inputs)\n","        x = BatchNormalization()(x)\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(x)\n","        x = BatchNormalization()(x)\n","        p = tf.keras.layers.MaxPool2D(pool_size=pool_size, strides=pool_size)(x)\n","        return x, p\n","    \n","    enc1, p1 = encoder_block(inputs, 64)\n","    enc2, p2 = encoder_block(p1, 128)\n","    enc3, p3 = encoder_block(p2, 256)\n","    enc4, p4 = encoder_block(p3, 512)\n","    enc5, p5 = encoder_block(p4, 512)\n","    \n","    # Decoder\n","    def decoder_block(inputs, skip, num_filters, up_size=(2, 2)):\n","        x = UpSampling2D(size=up_size)(inputs)\n","        x = Concatenate()([x, skip])\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(x)\n","        x = BatchNormalization()(x)\n","        x = Conv2D(num_filters, 3, padding=\"same\", activation='relu')(x)\n","        x = BatchNormalization()(x)\n","        return x\n","    \n","    dec1 = decoder_block(p5, enc5, 512)\n","    dec2 = decoder_block(dec1, enc4, 512)\n","    dec3 = decoder_block(dec2, enc3, 256)\n","    dec4 = decoder_block(dec3, enc2, 128)\n","    dec5 = decoder_block(dec4, enc1, 64)\n","\n","    # Output\n","    outputs = Conv2D(num_classes, 1, activation=\"softmax\", padding=\"same\")(dec5)\n","    \n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Fonction d'entraînement\n","def train_model(model, train_ds, valid_ds, model_path, csv_path, num_epochs, lr):\n","    callbacks = [\n","        ModelCheckpoint(model_path, verbose=1, save_best_only=True, monitor=\"val_loss\"),\n","        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n","        CSVLogger(csv_path, append=True),\n","        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n","    ]\n","\n","    model.compile(\n","        loss=\"categorical_crossentropy\",\n","        optimizer=tf.keras.optimizers.Adam(lr),\n","        metrics=[iou, dice_coefficient, precision, recall]\n","    )\n","\n","    model.fit(\n","        train_ds,\n","        validation_data=valid_ds,\n","        epochs=num_epochs,\n","        callbacks=callbacks\n","    )\n","    model.save(model_path)\n","\n","def load_segmentation_models(model_dir):\n","  \"\"\"Charge les modèles de segmentation à partir du répertoire spécifié.\"\"\"\n","  unet_model = tf.keras.models.load_model(os.path.join(model_dir, \"unet_model.keras\"), custom_objects={'iou': iou, 'dice_coefficient': dice_coefficient, 'precision': precision, 'recall': recall})\n","  pspnet_model = tf.keras.models.load_model(os.path.join(model_dir, \"pspnet_model.keras\"), custom_objects={'iou': iou, 'dice_coefficient': dice_coefficient, 'precision': precision, 'recall': recall})\n","  segnet_model = tf.keras.models.load_model(os.path.join(model_dir, \"segnet_model.keras\"), custom_objects={'iou': iou, 'dice_coefficient': dice_coefficient, 'precision': precision, 'recall': recall})\n"," \n","\n","  assert unet_model is not None, \"Erreur lors du chargement du modèle U-Net.\"\n","  assert pspnet_model is not None, \"Erreur lors du chargement du modèle PSPNet.\"\n","  assert segnet_model is not None, \"Erreur lors du chargement du modèle SegNet.\"\n","  print(\"Tous les modèles sont correctement chargés.\")\n","\n","  return unet_model, pspnet_model, segnet_model\n","\n","\n","def load_and_preprocess_image(image_path, image_h=256, image_w=256):\n","    \"\"\"Charge, redimensionne et normalise l'image.\"\"\"\n","    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","    image = cv2.resize(image, (image_w, image_h))\n","    image = image / 255.0\n","    image = image.astype(np.float32)\n","    return image\n","\n","def display_mask(prediction, ax, cmap='viridis'):\n","    \"\"\"Affiche le masque de segmentation.\"\"\"\n","    mask = np.argmax(prediction, axis=-1)\n","    ax.imshow(mask, cmap=cmap)\n","    ax.axis('off')\n","\n","def visualize_predictions(models, model_names, dataset_path, num_images=4, use_random_images=False):\n","    \"\"\"Visualise les prédictions de plusieurs modèles.\"\"\"\n","    if use_random_images:\n","      # Choisir aléatoirement des images\n","      all_image_paths = sorted(glob(os.path.join(dataset_path, \"val\", \"images\", \"*.jpg\")))\n","      test_image_paths = np.random.choice(all_image_paths, size=num_images, replace=False)\n","    else:\n","      # Utiliser les premières images du dossier\n","      test_image_paths = sorted(glob(os.path.join(dataset_path, \"val\", \"images\", \"*.jpg\")))[:num_images]\n","\n","    # Affichage des prédictions\n","    fig, axes = plt.subplots(num_images, len(models) + 1, figsize=(5*(len(models)+1), 5 * num_images))\n","    fig.suptitle(\"Comparaison des prédictions des modèles\", fontsize=16)\n","\n","    for i, image_path in enumerate(test_image_paths):\n","        # Charger et prétraiter l'image\n","        image = load_and_preprocess_image(image_path)\n","        image = np.expand_dims(image, axis=0)  # Ajouter une dimension pour le batch\n","        \n","        # Afficher l'image originale\n","        axes[i, 0].imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n","        axes[i, 0].axis('off')\n","        axes[i, 0].set_title(\"Image originale\")\n","        \n","        # Faire la prédiction et afficher le résultat\n","        for j, (model, model_name) in enumerate(zip(models, model_names)):\n","            prediction = model.predict(image)\n","            display_mask(prediction[0], axes[i, j + 1])  # prediction[0] car predict retourne un batch de taille 1\n","            axes[i, j + 1].set_title(f\"Masque prédit ({model_name})\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Programme principal\n","if __name__ == \"__main__\":\n","    # Hyperparamètres\n","    image_h, image_w = 256, 256  # Réduction de la taille des images\n","    num_classes = 11\n","    batch_size = 8\n","    lr = 1e-4\n","    num_epochs = 10\n","\n","    # Chemins\n","    dataset_path = \"/kaggle/input/lapa-face-parsing-dataset/LaPa\"\n","    create_dir(\"files\")\n","\n","    # Chargement du dataset\n","    (train_x, train_y), (valid_x, valid_y) = load_dataset(dataset_path)\n","\n","    # Création des datasets TensorFlow\n","    train_ds = tf_dataset(train_x, train_y, batch_size, image_h, image_w, num_classes)\n","    valid_ds = tf_dataset(valid_x, valid_y, batch_size, image_h, image_w, num_classes)\n","\n","\n","    # Modèles\n","    models = {\n","        \"unet\": build_unet,\n","        \"pspnet\": build_pspnet,\n","        \"segnet\": build_segnet,\n","    }\n","\n","    # Entraînement de chaque modèle\n","    for model_name, model_builder in models.items():\n","        print(f\"Entraînement de {model_name}...\")\n","        model = model_builder((image_h, image_w, 3), num_classes)\n","        model_path = os.path.join(\"files\", f\"{model_name}_model.keras\")\n","        csv_path = os.path.join(\"files\", f\"{model_name}_data.csv\")\n","        train_model(model, train_ds, valid_ds, model_path, csv_path, num_epochs, lr)"]},{"cell_type":"markdown","metadata":{},"source":["## *Visualisation des Prédictions des Modèles de Segmentation Faciale*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-01-19T01:51:36.848019Z","iopub.status.busy":"2025-01-19T01:51:36.847663Z","iopub.status.idle":"2025-01-19T01:52:07.407943Z","shell.execute_reply":"2025-01-19T01:52:07.406970Z","shell.execute_reply.started":"2025-01-19T01:51:36.847992Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from glob import glob\n","\n","def load_and_preprocess_image(image_path, image_h=256, image_w=256):\n","    \"\"\"Charge, redimensionne et normalise l'image.\"\"\"\n","    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n","    image = cv2.resize(image, (image_w, image_h))\n","    image = image / 255.0\n","    image = image.astype(np.float32)\n","    return image\n","\n","def display_mask(prediction, ax, cmap='viridis', num_classes=11):\n","    \"\"\"Affiche le masque de segmentation avec une palette de couleurs adaptée.\"\"\"\n","    mask = np.argmax(prediction, axis=-1)\n","    \n","    # Créer une palette de couleurs personnalisée pour les différentes parties du visage\n","    colors = plt.cm.get_cmap('tab20')(np.linspace(0, 1, num_classes))\n","    custom_cmap = plt.cm.colors.ListedColormap(colors)\n","    \n","    im = ax.imshow(mask, cmap=custom_cmap, vmin=0, vmax=num_classes-1)\n","    ax.axis('off')\n","    return im\n","\n","def visualize_predictions(models, model_names, image_paths, save_path=None):\n","    \"\"\"\n","    Visualise les prédictions de plusieurs modèles.\n","    \n","    Args:\n","        models: Liste des modèles à comparer\n","        model_names: Liste des noms des modèles\n","        image_paths: Liste des chemins d'images à tester\n","        save_path: Chemin où sauvegarder les visualisations (optionnel)\n","    \"\"\"\n","    num_images = len(image_paths)\n","    \n","    # Création de la figure avec une taille adaptée\n","    fig, axes = plt.subplots(num_images, len(models) + 1, \n","                            figsize=(5*(len(models)+1), 5 * num_images))\n","    fig.suptitle(\"Comparaison des prédictions des modèles de segmentation\", \n","                 fontsize=16, y=1.02)\n","    \n","    # Ajuster axes pour une seule image\n","    if num_images == 1:\n","        axes = axes.reshape(1, -1)\n","\n","    # Pour chaque image\n","    for i, image_path in enumerate(image_paths):\n","        # Charger et prétraiter l'image\n","        image = load_and_preprocess_image(image_path)\n","        image_batch = np.expand_dims(image, axis=0)\n","        \n","        # Afficher l'image originale\n","        axes[i, 0].imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n","        axes[i, 0].axis('off')\n","        axes[i, 0].set_title(\"Image originale\", pad=10)\n","        \n","        # Faire la prédiction avec chaque modèle\n","        for j, (model, model_name) in enumerate(zip(models, model_names)):\n","            prediction = model.predict(image_batch, verbose=0)\n","            im = display_mask(prediction[0], axes[i, j + 1])\n","            axes[i, j + 1].set_title(f\"{model_name}\", pad=10)\n","    \n","    # Ajuster l'espacement\n","    plt.tight_layout()\n","    \n","    # Ajouter une colorbar pour la légende\n","    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n","    cbar = plt.colorbar(im, cax=cbar_ax)\n","    cbar.set_label('Classes de segmentation', rotation=270, labelpad=15)\n","    \n","    # Sauvegarder si un chemin est spécifié\n","    if save_path:\n","        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n","        print(f\"Visualisation sauvegardée dans {save_path}\")\n","    \n","    plt.show()\n","\n","# Fonction pour charger les modèles\n","def load_segmentation_models(model_dir):\n","    \"\"\"Charge les modèles de segmentation à partir du répertoire spécifié.\"\"\"\n","    # Définir les objets personnalisés pour le chargement\n","    custom_objects = {\n","        'iou': iou,\n","        'dice_coefficient': dice_coefficient,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","    \n","    # Charger les modèles\n","    unet_model = tf.keras.models.load_model(\n","        os.path.join(model_dir, \"unet_model.keras\"),\n","        custom_objects=custom_objects\n","    )\n","    \n","    pspnet_model = tf.keras.models.load_model(\n","        os.path.join(model_dir, \"pspnet_model.keras\"),\n","        custom_objects=custom_objects\n","    )\n","    \n","    segnet_model = tf.keras.models.load_model(\n","        os.path.join(model_dir, \"segnet_model.keras\"),\n","        custom_objects=custom_objects\n","    )\n","\n","    print(\"Tous les modèles sont correctement chargés.\")\n","    return unet_model, pspnet_model, segnet_model\n","\n","# Code principal pour l'utilisation\n","def main():\n","    model_dir = \"files\"\n","    dataset_path = \"/kaggle/input/lapa-face-parsing-dataset/LaPa\" \n","    # Charger les modèles\n","    unet_model, pspnet_model, segnet_model = load_segmentation_models(model_dir)\n","    models = [unet_model, pspnet_model, segnet_model]\n","    model_names = [\"U-Net\", \"PSPNet\", \"SegNet\"]\n","    \n","    test_images = sorted(glob(os.path.join(dataset_path, \"val\", \"images\", \"*.jpg\")))[5:6]\n","    \n","    results_dir = \"resultats_segmentation\"\n","    if not os.path.exists(results_dir):\n","        os.makedirs(results_dir)\n","    \n","    # Visualiser les prédictions\n","    visualize_predictions(\n","        models,\n","        model_names,\n","        test_images,\n","        save_path=os.path.join(results_dir, \"comparaison_modeles.png\") # Sauvegarder l'image\n","    )\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4705085,"sourceId":7992059,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
